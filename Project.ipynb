{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as Utils\n",
    "from dqn_agent import DQN_Agent\n",
    "from agent_handler import Agent_handler\n",
    "from assignment3_utils import *\n",
    "import numpy as np\n",
    "import gym\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box(0, 255, (210, 160, 3), uint8)\n",
      "Observation space size:  (210, 160, 3)\n",
      "Number of actions:  9\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MsPacman-v4', render_mode='rgb_array')\n",
    "\n",
    "num_obs, num_actions = Utils.describe_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(batch, update):\n",
    "  params = {\n",
    "    \"num_obs\": (4, 86, 80),\n",
    "    \"num_actions\": num_actions,\n",
    "    \"update_rate\": update,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"discount_factor\": 0.95,\n",
    "    \"exploration_factor\": 1,\n",
    "    \"min_exploration_rate\": 0.05,\n",
    "    \"exploration_decay\": 0.995,\n",
    "    \"batch_size\": batch,\n",
    "    \"name\": f'{batch}_{update}'\n",
    "  }\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "\n",
    "batchs = [8, 16]\n",
    "updates = [3, 10]\n",
    "\n",
    "for batch in batchs:\n",
    "  for update in updates:\n",
    "    agents.append(DQN_Agent(get_params(batch, update)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Agent_handler({\n",
    "    \"num_episodes\":10,\n",
    "    \"max_steps\":100,\n",
    "    \"notify_percent\":1,\n",
    "    \"skip\": 85,\n",
    "    \"checkpoint_interval\": 200,\n",
    "    \"crop\": {\n",
    "      \"top\": 0,\n",
    "      \"bottom\": -39,\n",
    "      \"left\": 0,\n",
    "      \"right\": -1,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Training Agent: 1/1 ~~~\n",
      "\tName\t: 8_3\n",
      "\tStart\t: 12-06 02:55\n",
      "\n",
      "\tEpisode\t: 0/10 0%\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4999e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.2309e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.1454e-13\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8487e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5969e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.6483e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7081e-12\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.0732e-10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.9868e-12\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.1835e-13\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8375e-13\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4088e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7798e-10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7072e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3520e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4105e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7842e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3439e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4531e-10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8190e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4689e-10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.4818e-10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.9641e-13\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7646e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3413e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4194e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.6514e-13\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6666e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4522e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3475e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4249e-10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4491e-10\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3969e-10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3971e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5416e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3578e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7439e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.2541e-10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.0407e-13\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4155e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4149e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4175e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3995e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3845e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3594e-10\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.3724e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.6614e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.5496e-12\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.3511e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.9016e-13\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9348e-13\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.9484e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4254e-10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.6264e-13\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.4822e-13\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7646e-13\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.1643e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.4213e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.6576e-12\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.3501e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0204e-10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0683e-12\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.1974e-13\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7578e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4431e-10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.0798e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2638e-12\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3924e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5473e-10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.1011e-13\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.1131e-13\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4309e-10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4337e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8515e-10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6780e-10\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.6485e-13\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.9601e-13\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.8903e-13\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3430e-10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.3484e-10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4287e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3363e-10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.6328e-13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\Project.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Source/AI%26ML/RL/Reinforcement-Learning/Project.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m handler\u001b[39m.\u001b[39;49mtrain([agents[\u001b[39m0\u001b[39;49m]], env)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\agent_handler.py:132\u001b[0m, in \u001b[0;36mAgent_handler.train\u001b[1;34m(self, agents, env)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mName\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00magent\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    130\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mStart\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m average_steps, average_rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_agent(agent, env)\n\u001b[0;32m    134\u001b[0m results[agent\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m: average_steps, \u001b[39m\"\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m\"\u001b[39m: average_rewards}\n\u001b[0;32m    136\u001b[0m agent\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\agent_handler.py:76\u001b[0m, in \u001b[0;36mAgent_handler.train_agent\u001b[1;34m(self, agent, env)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m   \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mselect_action(state)\n\u001b[0;32m     77\u001b[0m     next_frame, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     78\u001b[0m     reward \u001b[39m=\u001b[39m transform_reward(reward)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\dqn_agent.py:138\u001b[0m, in \u001b[0;36mDQN_Agent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    136\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_actions)\n\u001b[0;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselect_greedy_action(state)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\dqn_agent.py:142\u001b[0m, in \u001b[0;36mDQN_Agent.select_greedy_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_greedy_action\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[1;32m--> 142\u001b[0m   q_values_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_network\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49mexpand_dims(state, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    143\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(q_values_state)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = handler.train([agents[0]], env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.json', 'r') as file:\n",
    "    results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "\n",
    "for agent, result in results.items():\n",
    "  moving_average = np.convolve(result[\"rewards\"], np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "  padding = np.full(5 - 1, 0)\n",
    "  result_array = np.concatenate([padding, moving_average])\n",
    "  result[\"rewards averages\"] = result_array\n",
    "  Utils.plot_results(result, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
