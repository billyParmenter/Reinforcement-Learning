{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as Utils\n",
    "from dqn_agent import DQN_Agent\n",
    "from agent_handler import Agent_handler\n",
    "from double_dqn_agent import Double_DQN_Agent\n",
    "from dueling_dqn_agent import DuelingDQN_Agent\n",
    "from assignment3_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box(0, 255, (210, 160, 3), uint8)\n",
      "Observation space size:  (210, 160, 3)\n",
      "Number of actions:  6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongDeterministic-v4', render_mode='rgb_array')\n",
    "\n",
    "num_obs, num_actions = Utils.describe_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "\n",
    "obs = (4, 81, 70, 1)\n",
    "\n",
    "batch = 8\n",
    "update = 10\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))\n",
    "\n",
    "batch = 16\n",
    "update = 10\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))\n",
    "\n",
    "batch = 8\n",
    "update = 3\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))\n",
    "\n",
    "batch = 16\n",
    "update = 3\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Agent_handler({\n",
    "    \"num_episodes\":500,\n",
    "    \"max_steps\":500,\n",
    "    \"notify_percent\":5,\n",
    "    \"skip\": 15,\n",
    "    \"checkpoint_interval\": 10,\n",
    "    \"crop\": {\n",
    "      \"top\": 33,\n",
    "      \"bottom\": -16,\n",
    "      \"left\": 20,\n",
    "      \"right\": -1,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Training Agent: 1/4 ~~~\n",
      "\tName\t: 8_10\n",
      "\tStart\t: 11-27 11:04\n",
      "\n",
      "\tEpisode\t: 0/500 0%\n",
      "\tBetter\t: -10.0 ~ 11-27 11:04\n",
      "\tBetter\t: -8.0 ~ 11-27 11:04\n",
      "\tCheckpoint: 11-27 11:04\n",
      "\tBetter\t: -3.0 ~ 11-27 11:04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\Project.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Source/AI%26ML/RL/Reinforcement-Learning/Project.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m results \u001b[39m=\u001b[39m handler\u001b[39m.\u001b[39;49mtrain(agents, env)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\agent_handler.py:121\u001b[0m, in \u001b[0;36mAgent_handler.train\u001b[1;34m(self, agents, env)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mName\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00magent\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mStart\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m average_steps, average_rewards \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_agent(agent, env)\n\u001b[0;32m    123\u001b[0m results[agent\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39msteps\u001b[39m\u001b[39m\"\u001b[39m: average_steps, \u001b[39m\"\u001b[39m\u001b[39mrewards\u001b[39m\u001b[39m\"\u001b[39m: average_rewards}\n\u001b[0;32m    125\u001b[0m agent\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mfinal\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\agent_handler.py:80\u001b[0m, in \u001b[0;36mAgent_handler.train_agent\u001b[1;34m(self, agent, env)\u001b[0m\n\u001b[0;32m     77\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     78\u001b[0m steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 80\u001b[0m agent\u001b[39m.\u001b[39;49mupdate_target_network(episode)\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39mor\u001b[39;00m steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_steps:\n\u001b[0;32m     84\u001b[0m   episode_steps\u001b[39m.\u001b[39mappend(steps)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\RL\\Reinforcement-Learning\\dqn_agent.py:89\u001b[0m, in \u001b[0;36mDQN_Agent.update_target_network\u001b[1;34m(self, episode)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_target_network\u001b[39m(\u001b[39mself\u001b[39m, episode):\n\u001b[0;32m     88\u001b[0m   \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_rate \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_q_network\u001b[39m.\u001b[39mset_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_network\u001b[39m.\u001b[39;49mget_weights())\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\keras\\engine\\training.py:2770\u001b[0m, in \u001b[0;36mModel.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Retrieves the weights of the model.\u001b[39;00m\n\u001b[0;32m   2765\u001b[0m \n\u001b[0;32m   2766\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[0;32m   2767\u001b[0m \u001b[39m    A flat list of Numpy arrays.\u001b[39;00m\n\u001b[0;32m   2768\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2769\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 2770\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mget_weights()\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\keras\\engine\\base_layer.py:1883\u001b[0m, in \u001b[0;36mLayer.get_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1881\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1882\u001b[0m         output_weights\u001b[39m.\u001b[39mappend(weight)\n\u001b[1;32m-> 1883\u001b[0m \u001b[39mreturn\u001b[39;00m backend\u001b[39m.\u001b[39;49mbatch_get_value(output_weights)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\keras\\backend.py:4249\u001b[0m, in \u001b[0;36mbatch_get_value\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   4237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[0;32m   4238\u001b[0m \n\u001b[0;32m   4239\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4246\u001b[0m \u001b[39m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[0;32m   4247\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4248\u001b[0m \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m-> 4249\u001b[0m     \u001b[39mreturn\u001b[39;00m [x\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tensors]\n\u001b[0;32m   4250\u001b[0m \u001b[39melif\u001b[39;00m tf\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m   4251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\keras\\backend.py:4249\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   4237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the value of more than one tensor variable.\u001b[39;00m\n\u001b[0;32m   4238\u001b[0m \n\u001b[0;32m   4239\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4246\u001b[0m \u001b[39m    RuntimeError: If this method is called inside defun.\u001b[39;00m\n\u001b[0;32m   4247\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4248\u001b[0m \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m-> 4249\u001b[0m     \u001b[39mreturn\u001b[39;00m [x\u001b[39m.\u001b[39;49mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tensors]\n\u001b[0;32m   4250\u001b[0m \u001b[39melif\u001b[39;00m tf\u001b[39m.\u001b[39minside_function():\n\u001b[0;32m   4251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot get value inside Tensorflow graph function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:647\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    646\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 647\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    648\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    649\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Source\\AI&ML\\FML\\CSCN8010\\venv\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1161\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1161\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39;49mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = handler.train(agents, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "\n",
    "for agent, result in results.items():\n",
    "  moving_average = np.convolve(result[\"rewards\"], np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "  padding = np.full(5 - 1, 0)\n",
    "  result_array = np.concatenate([padding, moving_average])\n",
    "  result[\"rewards averages\"] = result_array\n",
    "  Utils.plot_results(result, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
