{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils as Utils\n",
    "from dqn_agent import DQN_Agent\n",
    "from agent_handler import Agent_handler\n",
    "from double_dqn_agent import Double_DQN_Agent\n",
    "from dueling_dqn_agent import DuelingDQN_Agent\n",
    "from assignment3_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space:  Box(0, 255, (210, 160, 3), uint8)\n",
      "Observation space size:  (210, 160, 3)\n",
      "Number of actions:  6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongDeterministic-v4', render_mode='rgb_array')\n",
    "\n",
    "num_obs, num_actions = Utils.describe_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = []\n",
    "\n",
    "obs = (4, 81, 70, 1)\n",
    "\n",
    "batch = 8\n",
    "update = 10\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))\n",
    "\n",
    "batch = 16\n",
    "update = 10\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))\n",
    "\n",
    "batch = 8\n",
    "update = 3\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))\n",
    "\n",
    "batch = 16\n",
    "update = 3\n",
    "options = {\n",
    "  \"num_obs\": obs,\n",
    "  \"num_actions\": num_actions,\n",
    "  \"update_rate\": update,\n",
    "  \"learning_rate\": 0.0001,\n",
    "  \"discount_factor\": 0.95,\n",
    "  \"exploration_factor\": 1,\n",
    "  \"min_exploration_rate\": 0.05,\n",
    "  \"exploration_decay\": 0.995,\n",
    "  \"batch_size\": batch,\n",
    "  \"name\": f'{batch}_{update}'\n",
    "}\n",
    "agents.append(DQN_Agent(options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Agent_handler({\n",
    "    \"num_episodes\":500,\n",
    "    \"max_steps\":500,\n",
    "    \"notify_percent\":5,\n",
    "    \"skip\": 15,\n",
    "    \"checkpoint_interval\": 10,\n",
    "    \"crop\": {\n",
    "      \"top\": 33,\n",
    "      \"bottom\": -16,\n",
    "      \"left\": 20,\n",
    "      \"right\": -1,\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Training Agent: 1/4 ~~~\n",
      "\tName\t: 8_10\n",
      "\tStart\t: 11-27 11:04\n",
      "\n",
      "\tEpisode\t: 0/500 0%\n",
      "\tBetter\t: -10.0 ~ 11-27 11:04\n",
      "\tBetter\t: -8.0 ~ 11-27 11:04\n",
      "\tCheckpoint: 11-27 11:04\n",
      "\tBetter\t: -3.0 ~ 11-27 11:04\n"
     ]
    }
   ],
   "source": [
    "results = handler.train(agents, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "\n",
    "for agent, result in results.items():\n",
    "  moving_average = np.convolve(result[\"rewards\"], np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "  padding = np.full(5 - 1, 0)\n",
    "  result_array = np.concatenate([padding, moving_average])\n",
    "  result[\"rewards averages\"] = result_array\n",
    "  Utils.plot_results(result, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
